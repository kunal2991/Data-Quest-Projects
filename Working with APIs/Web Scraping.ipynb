{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html>\\n    <head>\\n        <title>A simple example page</title>\\n    </head>\\n    <body>\\n        <p>Here is some simple content for this page.</p>\\n    </body>\\n</html>'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('http://dataquestio.github.io/web-scraping-pages/simple.html')\n",
    "content = response.content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping - Using requests library to fetch a website and with the content attribute, you can see it looks exactly like the source code of the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "parser = BeautifulSoup(content,'html.parser')\n",
    "body = parser.body\n",
    "\n",
    "p = body.p\n",
    "print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting individual content or 'scraping' the website, we are using the BeautifulSoup library. Here, we use the 'html.parser' parser to parse through the website.\n",
    "'.text' is the text proprety to retrieve contents from a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A simple example page\n"
     ]
    }
   ],
   "source": [
    "head = parser.head\n",
    "title = head.title\n",
    "title_text = title.text\n",
    "print(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to get data from the title tag. If the tag to be retrieved is nested within a tag, you need to first parse through the root tag and then access the inner tags till you reach the destination tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "#Using find_all method to find all occurences of a particular tag\n",
    "\n",
    "body = parser.find_all('body')\n",
    "p = body[0].find_all('p')\n",
    "print(p[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A simple example page\n"
     ]
    }
   ],
   "source": [
    "#Similarly for title to retrieve text within the title tag\n",
    "\n",
    "head = parser.find_all('head')\n",
    "title = head[0].find_all('title')\n",
    "print(title[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_ids.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "first_paragraph = parser.find_all('p',id='first')[0]\n",
    "print(first_paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a web page is divided into logical entities using the 'div' tag. And pages can have Element IDs. So we need to pass an additional attribute called 'id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                Second paragraph.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_paragraph = parser.find_all('p',id='second')[0]\n",
    "second_paragraph_text = second_paragraph.text\n",
    "print(second_paragraph_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n",
      "\n",
      "                Second paragraph.\n",
      "            \n",
      "\n",
      "\n",
      "                First outer paragraph.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "#Finding the first Inner Paragraph using the class_ parameter\n",
    "first_inner_paragraph = parser.find_all('p',class_='inner-text')[0]\n",
    "print(first_inner_paragraph.text)\n",
    "\n",
    "#Finding the second Inner Paragraph using the class_ parameter\n",
    "second_inner_paragraph = parser.find_all('p',class_='inner-text')[1]\n",
    "second_inner_paragraph_text = second_inner_paragraph.text\n",
    "print(second_inner_paragraph_text)\n",
    "\n",
    "#Finding the first Outer Paragraph using the class_ parameter\n",
    "first_outer_paragraph = parser.find_all('p',class_='outer-text')[0]\n",
    "first_outer_paragraph_text = first_outer_paragraph.text\n",
    "print(first_outer_paragraph_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements can also be divided into Classes. Classes aren't globally unique but usually indicate that elements are linked.\n",
    "One element can have or belong to multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n",
      "\n",
      "\n",
      "                First outer paragraph.\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "                First outer paragraph.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "#Select all elements with the 'first-item' class.\n",
    "first_items = parser.select('.first-item')\n",
    "print(first_items[0].text)\n",
    "\n",
    "#Select all elements with the 'outer-text' class.\n",
    "outer_text = parser.select('.outer-text')\n",
    "first_outer_text = outer_text[0].text\n",
    "print(first_outer_text)\n",
    "\n",
    "#Select all elements with the id = second\n",
    "second_id = parser.select('#second')\n",
    "second_text = second_id[0].text\n",
    "print(second_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSS Selectors are used to assign style to a particular element or class. We can use these CSS selectors to filter data using the .select method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "72\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/2014_super_bowl.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "turnovers = parser.select(\"#turnovers\")[0]\n",
    "seahawks_turnovers = turnovers.select(\"td\")[1]\n",
    "seahawks_turnovers_count = seahawks_turnovers.text\n",
    "print(seahawks_turnovers_count)\n",
    "\n",
    "total_plays = parser.select('#total-plays')[0]\n",
    "patriot_total_plays = total_plays.select('td')[2]\n",
    "patriot_total_plays_count = patriot_total_plays.text\n",
    "print(patriot_total_plays_count)\n",
    "\n",
    "total_yards = parser.select('#total-yards')[0]\n",
    "seahawks_total_yards = total_yards.select('td')[1]\n",
    "seahawks_total_yards_count = seahawks_total_yards.text\n",
    "print(seahawks_total_yards_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can nest the CSS selectors for parsing the data more efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
