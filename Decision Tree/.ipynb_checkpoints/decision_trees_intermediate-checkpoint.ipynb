{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education_num  \\\n",
      "0   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "1   38            Private  215646     HS-grad              9   \n",
      "2   53            Private  234721        11th              7   \n",
      "3   28            Private  338409   Bachelors             13   \n",
      "4   37            Private  284582     Masters             14   \n",
      "\n",
      "        marital_status          occupation    relationship    race      sex  \\\n",
      "0   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "1             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "2   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "3   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "4   Married-civ-spouse     Exec-managerial            Wife   White   Female   \n",
      "\n",
      "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
      "0             0             0              13   United-States       <=50K  \n",
      "1             0             0              40   United-States       <=50K  \n",
      "2             0             0              40   United-States       <=50K  \n",
      "3             0             0              40            Cuba       <=50K  \n",
      "4             0             0              40   United-States       <=50K  \n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "income = pandas.read_csv('income.csv',index_col=False)\n",
    "columns = [['age','workclass','fnlwgt','education','education_num','marital_status','occupation','relationship','race','sex','capital_gain','capital_loss','hours_per_week','native_country','high_income']]\n",
    "income.columns = columns\n",
    "print(income.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6\n",
      "1    4\n",
      "2    4\n",
      "3    4\n",
      "4    4\n",
      "Name: workclass, dtype: int8\n"
     ]
    }
   ],
   "source": [
    "#Converting categorical variables in income to codes using Categorical.from_array\n",
    "\n",
    "col = pandas.Categorical.from_array(income['workclass'])\n",
    "income['workclass'] = col.codes\n",
    "print(income['workclass'].head(5))\n",
    "for name in [\"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\", \"high_income\"]:\n",
    "    col = pandas.Categorical.from_array(income[name])\n",
    "    income[name] = col.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using sklearn.tree package to fit the decision tree. Here we are using the DecisionTreeClassifier class.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "#Instantiating the classifier\n",
    "clf = DecisionTreeClassifier(random_state = 1)\n",
    "clf.fit(income[columns], income['high_income']) #Fit() method to fit the Decision tree to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting our data into TEST and TRAIN data sets. Here we use 80% of our data as Train and remaning as Test\n",
    "\n",
    "import math\n",
    "np.random.seed(1)\n",
    "income = income.reindex(np.random.permutation(income.index))\n",
    "\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "\n",
    "train = income[:train_max_row]\n",
    "test = income[train_max_row:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.702929759027\n"
     ]
    }
   ],
   "source": [
    "#Using AUC to evaluate error. Note: Higher the AUC, more accurate the predictions. AUC ranges from 0 to 1.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "error = roc_auc_score(predictions, test['high_income'])\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the error is 0.7 on the test set. Next, we check the AUC score for the fit on the training set to see if we are overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97191113789\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train[columns])\n",
    "error = roc_auc_score(predictions, train['high_income'])\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from the above value that we are infact overfitting the model since we are getting a AUC score of 0.97 for the training set.\n",
    "Note: Trees overfit when they have too much depth i.e more depth the tree has, the worse it performs on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93200392174\n",
      "0.716606685283\n"
     ]
    }
   ],
   "source": [
    "#Improving model by restricting the depth of the tree while building it.\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_split = 5)\n",
    "clf.fit(train[columns], train['high_income'])\n",
    "predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(predictions, train['high_income'])\n",
    "print(train_auc)\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(predictions, test['high_income'])\n",
    "print(test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tweaking the min_samples_fit parameter, we managed to improve our test auc score from 0.703 to 0.716 and managed to reduce train auc score from 0.97 to 0.93 thereby reducing the overfit by some amount. Continuing to tweak the model parameters further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802536734965\n",
      "0.793488947036\n"
     ]
    }
   ],
   "source": [
    "#Tweaking the max_depth and min_samples_split properties.\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1, max_depth=4, min_samples_split=25)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(predictions, test[\"high_income\"])\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train_predictions, train[\"high_income\"])\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test auc score is now 0.80 which means that we have improved it much further. Also, the train auc score is now 0.79 which means that we are not overfitting anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78017266921\n",
      "0.773555652556\n"
     ]
    }
   ],
   "source": [
    "#Tweaking the parameters more aggressively. Trying to understand \"Underfitting\".\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1, max_depth=2, min_samples_split=100)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(predictions, test[\"high_income\"])\n",
    "\n",
    "train_predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(train_predictions, train[\"high_income\"])\n",
    "\n",
    "print(test_auc)\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our test auc score went down from 0.8 to 0.78. This is because we are now \"underfitting\". It means that our model is \"too simple\" to actually explain the relations between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698470862598\n",
      "0.990088832083\n"
     ]
    }
   ],
   "source": [
    "#Exploring Decision Tree Variance\n",
    "\n",
    "np.random.seed(1)\n",
    "#Introducing random noise\n",
    "income[\"noise\"] = np.random.randint(4, size=income.shape[0])\n",
    "\n",
    "columns = [\"noise\", \"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "train_max_row = math.floor(income.shape[0] * .8)\n",
    "train = income.iloc[:train_max_row]\n",
    "test = income.iloc[train_max_row:]\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(train[columns], train['high_income'])\n",
    "predictions = clf.predict(test[columns])\n",
    "test_auc = roc_auc_score(predictions, test['high_income'])\n",
    "print(test_auc)\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "train_auc = roc_auc_score(predictions, train['high_income'])\n",
    "print(train_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above code cell we can see that when we introduce random noise, it caused significant overfitting in the model i.e the auc score for the train data went up from 0.77 to 0.99."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
