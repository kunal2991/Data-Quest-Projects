{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18.0  8  307.0  130.0   3504.  12.0  70  1 chevrolet chevelle malibu\n",
      "0  15.0  8  350.0  165.0  3693.0  11.5  70  1         buick skylark 320\n",
      "1  18.0  8  318.0  150.0  3436.0  11.0  70  1        plymouth satellite\n",
      "2  16.0  8  304.0  150.0  3433.0  12.0  70  1             amc rebel sst\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "mpg = pd.read_csv('auto-mpg.data.txt',delim_whitespace=True)\n",
    "print(mpg.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "3  16.0          8         304.0      150.0  3433.0          12.0          70   \n",
      "4  17.0          8         302.0      140.0  3449.0          10.5          70   \n",
      "\n",
      "   origin                   car name  \n",
      "0       1  chevrolet chevelle malibu  \n",
      "1       1          buick skylark 320  \n",
      "2       1         plymouth satellite  \n",
      "3       1              amc rebel sst  \n",
      "4       1                ford torino  \n"
     ]
    }
   ],
   "source": [
    "columns = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\", \"origin\", \"car name\"]\n",
    "cars = pd.read_table(\"auto-mpg.data.txt\", delim_whitespace=True, names=columns)\n",
    "print(cars.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalbarde/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "filtered_cars = cars[cars['horsepower'] != '?']\n",
    "filtered_cars['horsepower'] = filtered_cars['horsepower'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of understanding overfitting is understanding bias and variance. Bias and variance make up the 2 observable sources of error in a model that we can indirectly control.\n",
    "\n",
    "Bias describes error that results in bad assumptions about the learning algorithm. For example, assuming that only one feature, like a car's weight, relates to a car's fuel efficiency will lead you to fit a simple, univariate regression model that will result in high bias. The error rate will be high since a car's fuel efficiency is affected by many other factors besides just its weight.\n",
    "\n",
    "Variance describes error that occurs because of the variability of a model's predicted values. If we were given a dataset with 1000 features on each car and used every single feature to train an incredibly complicated multivariate regression model, we will have low bias but high variance.\n",
    "\n",
    "In an ideal world, we want low bias and low variance but in reality, there's always a tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0201795682 36.7425588742\n",
      "18.6766165974 42.0861218449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_and_test(cols):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(filtered_cars[cols],filtered_cars['mpg'])\n",
    "    predictions = lr.predict(filtered_cars[cols])\n",
    "    mse = mean_squared_error(filtered_cars['mpg'],predictions)\n",
    "    variance = np.var(predictions)\n",
    "    return(mse,variance)\n",
    "\n",
    "cyl_mse, cyl_var = train_and_test(['cylinders'])\n",
    "weight_mse, weight_var = train_and_test(['weight'])\n",
    "print(cyl_mse,cyl_var)\n",
    "print(weight_mse,weight_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can approximate the bias of a model by training a few different models from the same class (linear regression in this case) using different features on the same dataset and calculating their error scores. For regression, we can use mean absolute error, mean squared error, or R-squared.\n",
    "\n",
    "We can calculate the variance of the predicted values for each model we train and we'll observe an increase in variance as we build more complex, multivariate models.\n",
    "\n",
    "While an extremely simple, univariate linear regression model will underfit, an extremely complicated, multivariate linear regression model will overfit. Depending on the problem you're working on, there's a happy middle ground that will help you construct reliable and useful predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0201795682 36.7425588742\n",
      "21.2820570556 39.4806813867\n",
      "20.2529548397 40.5097836026\n",
      "17.7638605718 42.9988778705\n",
      "17.7613961054 43.0013423369\n",
      "11.5901709814 49.1725674609\n",
      "10.847480945 49.9152574973\n"
     ]
    }
   ],
   "source": [
    "one_mse, one_var = train_and_test([\"cylinders\"])\n",
    "two_mse, two_var = train_and_test([\"cylinders\", \"displacement\"])\n",
    "three_mse, three_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_test([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])\n",
    "print(one_mse, one_var)\n",
    "print(two_mse, two_var)\n",
    "print(three_mse, three_var)\n",
    "print(four_mse, four_var)\n",
    "print(five_mse, five_var)\n",
    "print(six_mse, six_var)\n",
    "print(seven_mse, seven_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function for training a regression model and calculating the mean squared error and variance, let's use it to train and understand more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multivariate regression models you trained got progressively better at reducing the amount of error.\n",
    "\n",
    "A good way to detect if your model is overfitting is to compare the in-sample error and the out-of-sample error, or the training error with the test error. So far, we calculated the in sample error by testing the model over the same data it was trained on. To calculate the out-of-sample error, we need to test the data on a test set of data. We unfortunately don't have a separate test dataset and we'll instead use cross validation.\n",
    "\n",
    "If a model's cross validation error (out-of-sample error) is much higher than the in sample error, then your data science senses should start to tingle. This is the first line of defense against overfitting and is a clear indicator that the trained model doesn't generalize well outside of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.584370275 38.9025253138\n",
      "20.6556221939 40.0912879566\n",
      "18.1696832391 42.5076436436\n",
      "18.2830385172 42.5987363001\n",
      "12.0996854255 48.9282469677\n",
      "11.4181319718 49.904313731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def train_and_cross_val(cols):\n",
    "    features = filtered_cars[cols]\n",
    "    target = filtered_cars[\"mpg\"]\n",
    "    \n",
    "    variance_values = []\n",
    "    mse_values = []\n",
    "    \n",
    "    kf = KFold(n=len(filtered_cars), n_folds=10, shuffle=True, random_state=3)\n",
    "    \n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "        \n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        predictions = lr.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        var = np.var(predictions)\n",
    "\n",
    "        variance_values.append(var)\n",
    "        mse_values.append(mse)\n",
    "   \n",
    "    avg_mse = np.mean(mse_values)\n",
    "    avg_var = np.mean(variance_values)\n",
    "    return(avg_mse, avg_var)\n",
    "        \n",
    "two_mse, two_var = train_and_cross_val([\"cylinders\", \"displacement\"])\n",
    "three_mse, three_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\"])\n",
    "four_mse, four_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\"])\n",
    "five_mse, five_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\"])\n",
    "six_mse, six_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model year\"])\n",
    "seven_mse, seven_var = train_and_cross_val([\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\"model year\", \"origin\"])\n",
    "print(two_mse, two_var)\n",
    "print(three_mse, three_var)\n",
    "print(four_mse, four_var)\n",
    "print(five_mse, five_var)\n",
    "print(six_mse, six_var)\n",
    "print(seven_mse, seven_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During cross validation, the more features we added to the model, the lower the mean squared error got. This is a good sign and indicates that the model generalizes well to new data it wasn't trained on. As the mean squared error value went up, however, so did the variance of the predictions. This is to be expected, since the models with lower squared error values had higher model complexity, which tends to be more sensitive to small variations in input values (or high variance).\n",
    "\n",
    "For each model, let's plot the error and variance to get a better idea of the tradeoff as the number of features increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAECCAYAAADjBlzIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11752feb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter([2,3,4,5,6,7], [two_mse, three_mse, four_mse, five_mse, six_mse, seven_mse], c='red')\n",
    "plt.scatter([2,3,4,5,6,7], [two_var, three_var, four_var, five_var, six_var, seven_var], c='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the higher order multivariate models overfit in relation to the lower order multivariate models, the in-sample error and out-of-sample didn't deviate by much. The best model was around 50% more accurate than the simplest model. On the other hand, the overall variance increased around 25% as we increased the model complexity. This is a really good starting point, but your work is not done! The increased variance with the increased model complexity means that your model will have more unpredictable performance on truly new, unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
